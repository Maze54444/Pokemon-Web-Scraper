import os
import requests
import json
import hashlib
import time
import re
from bs4 import BeautifulSoup
from datetime import datetime
import unicodedata


# ğŸ” Auto-Reset der seen.txt beim Start (optional)
if os.path.exists("seen.txt"):
    os.remove("seen.txt")
    print("ğŸ—‘ï¸ seen.txt gelÃ¶scht (beim Start)", flush=True)

def load_list(filename):
    with open(filename, "r", encoding="utf-8") as f:
        return [line.strip().lower() for line in f if line.strip()]

def load_seen():
    try:
        with open("seen.txt", "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    except FileNotFoundError:
        return set()

def save_seen(seen):
    with open("seen.txt", "w", encoding="utf-8") as f:
        for item in seen:
            f.write(item + "\n")

def load_telegram_config():
    with open("telegram_config.json", "r") as f:
        return json.load(f)

def send_telegram_message(text):
    try:
        config = load_telegram_config()
        url = f"https://api.telegram.org/bot{config['bot_token']}/sendMessage"
        data = {"chat_id": config["chat_id"], "text": text}
        response = requests.post(url, data=data)
        if response.status_code == 200:
            print("ğŸ“¨ Telegram-Nachricht gesendet", flush=True)
        else:
            print(f"âš ï¸ Telegram-Fehler: {response.status_code}", flush=True)
    except Exception as e:
        print(f"âŒ Telegram-Fehler: {e}", flush=True)

def clean_text(text):
    text = unicodedata.normalize("NFKD", text)
    text = text.encode("ascii", "ignore").decode("utf-8")  # Entfernt Akzente korrekt
    text = re.sub(r"[^a-zA-Z0-9 ]", " ", text)
    return text.lower()

def is_flexible_match(keywords, raw_text, threshold=0.75):
    text = clean_text(raw_text)
    text_words = set(text.split())
    keywords_clean = [clean_text(word) for word in keywords]
    matches = [word for word in keywords_clean if word in text_words]
    score = len(matches) / len(keywords_clean)
    print(f"ğŸŸ¡ LOG: Keywords = {keywords_clean}")
    print(f"ğŸŸ¡ LOG: Gefundene WÃ¶rter = {matches} â†’ Trefferquote = {score:.2f}")
    return score >= threshold

# ğŸ” JSON-Methode fÃ¼r tcgviert.com
def scrape_tcgviert(products, seen):
    print("ğŸŒ JSON-API von tcgviert.com wird verwendet", flush=True)
    try:
        response = requests.get("https://tcgviert.com/products.json", timeout=10)
        response.raise_for_status()
        data = response.json()

        for item in data["products"]:
            title = item["title"].lower()
            handle = item["handle"]
            link = f"https://tcgviert.com/products/{handle}"
            price = item["variants"][0]["price"]
            identifier = hashlib.md5((title + link).encode()).hexdigest()

            for entry in products:
                keywords = entry.lower().split()
                if is_flexible_match(keywords, title) and identifier not in seen:
                    seen.add(identifier)
                    message = f"ğŸ”¥ Neuer Fund: {item['title']}\nğŸ’¶ Preis: {price} â‚¬\nğŸ”— Link: {link}"
                    print(f"âœ… TREFFER: {item['title']} â€“ {price} â‚¬ â€“ {link}", flush=True)
                    send_telegram_message(message)

    except Exception as e:
        print(f"âŒ Fehler bei tcgviert.com: {e}", flush=True)

# ğŸŒ HTML-Methode (klassisch)
def scrape_generic(url, products, seen):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        text = soup.get_text().lower()

        for entry in products:
            keywords = entry.lower().split()
            identifier = hashlib.md5((entry + url).encode()).hexdigest()
            if is_flexible_match(keywords, text) and identifier not in seen:
                seen.add(identifier)
                message = f"ğŸ”¥ Neuer Fund auf generischer Seite:\nğŸ“ Produkt: {entry}\nğŸ”— Link: {url}"
                print(f"âœ… GENERISCH TREFFER: {entry} â†’ {url}", flush=True)
                send_telegram_message(message)

    except Exception as e:
        print(f"âŒ Fehler bei {url}: {e}", flush=True)

# ğŸ” Dauerhafter Zyklus
if __name__ == "__main__":
    print("ğŸŸ¢ Master-Scraper gestartet", flush=True)
    seen = load_seen()

    while True:
        urls = load_list("urls.txt")
        products = load_list("products.txt")

        for url in urls:
            if "tcgviert.com" in url:
                scrape_tcgviert(products, seen)
            else:
                scrape_generic(url, products, seen)

        save_seen(seen)
        print(f"â³ Warten bis {datetime.now().strftime('%H:%M:%S')} + 300s\n", flush=True)
        time.sleep(300)
